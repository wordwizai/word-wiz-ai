# Model Optimization Settings for Low-Resource Environments
# ========================================================

# Set to true to use optimized models for low-resource environments
USE_OPTIMIZED_PHONEME_MODEL=true

# Set to true to enable quantization (reduces model size and speeds up inference)
USE_MODEL_QUANTIZATION=true

# Set to true to use smaller, faster models instead of large ones
USE_FAST_MODELS=true

# Set to true to enable model compilation with torch.compile (if available)
USE_MODEL_COMPILATION=true

# Model compilation mode: 'reduce-overhead', 'default', 'max-autotune'
MODEL_COMPILATION_MODE=reduce-overhead

# Set to true to enable performance logging
ENABLE_PERFORMANCE_LOGGING=false

# Set to true to enable model caching (reduces load times on repeated use)
ENABLE_MODEL_CACHE=true

# Number of warmup runs to perform after model loading (0 to disable)
MODEL_WARMUP_RUNS=1

# Set to true to fallback to basic configuration if optimizations fail
FALLBACK_ON_OPTIMIZATION_ERROR=true

# Set to true to clear unnecessary cache after model initialization
CLEAR_CACHE_AFTER_INIT=false